{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLerSunny/LLM/blob/main/Fine_Tuning_of_LLMs_Complete_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning of LLMs with Hugging Face"
      ],
      "metadata": {
        "id": "WVa0caPZlogN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Installing and importing the libraries for Hugging Face"
      ],
      "metadata": {
        "id": "fT5BjFcflZAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "!pip uninstall accelerate peft bitsandbytes transformers trl -y\n",
        "!pip install accelerate peft==0.13.2 bitsandbytes transformers trl==0.12.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "eRZm_OAbs3qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging)"
      ],
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Setting up links to Hugging Face datasets and models"
      ],
      "metadata": {
        "id": "CMO9Ew1534fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_identifier = \"aboonaji/llama2finetune-v2\"\n",
        "source_dataset = \"gamino/wiki_medical_terms\"\n",
        "formatted_dataset = \"aboonaji/wiki_medical_terms_llam2_format\""
      ],
      "metadata": {
        "id": "2Vi4I3IR1Cjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Setting up all the QLoRA hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "OQA3fMY647SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_hyper_r = 64\n",
        "lora_hyper_alpha = 16\n",
        "lora_hyper_dropout = 0.1"
      ],
      "metadata": {
        "id": "ND2GwTK17Dyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Setting up all the bitsandbytes hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "YFfD46YB2ffY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enable_4bit = True\n",
        "compute_dtype_bnb = \"float16\"\n",
        "quant_type_bnb = \"nf4\"\n",
        "double_quant_flag = False"
      ],
      "metadata": {
        "id": "OkDBdROgmYw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Setting up all the training arguments hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "_66BRztm29wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_dir = \"./results\"\n",
        "epochs_count = 10\n",
        "enable_fp16 = False\n",
        "enable_bf16 = False\n",
        "train_batch_size = 4\n",
        "eval_batch_size = 4\n",
        "accumulation_steps = 1\n",
        "checkpointing_flag = True\n",
        "grad_norm_limit = 0.3\n",
        "train_learning_rate = 2e-4\n",
        "decay_rate = 0.001\n",
        "optimizer_type = \"paged_adamw_32bit\"\n",
        "scheduler_type = \"cosine\"\n",
        "steps_limit = 100\n",
        "warmup_percentage = 0.03\n",
        "length_grouping = True\n",
        "checkpoint_interval = 0\n",
        "log_interval = 25"
      ],
      "metadata": {
        "id": "C5UJVZlEDPE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Setting up all the supervised fine-tuning arguments hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "3cmmRIgc3Ree"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enable_packing = False\n",
        "sequence_length_max = None\n",
        "device_assignment = {\"\": 0}"
      ],
      "metadata": {
        "id": "yjj4vaC_eje2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Loading the dataset"
      ],
      "metadata": {
        "id": "QEsqc9gasp10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = load_dataset(formatted_dataset, split = \"train\")"
      ],
      "metadata": {
        "id": "XkiKtLen-jKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data"
      ],
      "metadata": {
        "id": "Z5kd6_Ad_Rlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Defining the QLoRA configuration"
      ],
      "metadata": {
        "id": "z0tKj05usvyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtype_computation = getattr(torch, compute_dtype_bnb)\n",
        "bnb_setup = BitsAndBytesConfig(load_in_4bit = enable_4bit,\n",
        "                               bnb_4bit_quant_type = quant_type_bnb,\n",
        "                               bnb_4bit_use_double_quant = double_quant_flag,\n",
        "                               bnb_4bit_compute_dtype = dtype_computation)"
      ],
      "metadata": {
        "id": "TE7P9j0MDcXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Loading the pre-trained LLaMA 2 model"
      ],
      "metadata": {
        "id": "rz3vMSzhs-P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_model = AutoModelForCausalLM.from_pretrained(model_identifier, quantization_config = bnb_setup, device_map = device_assignment)\n",
        "llama_model.config.use_case = False\n",
        "llama_model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "mS5OW4q2Orp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Loading the pre-trained tokenizer for the LLaMA 2 model"
      ],
      "metadata": {
        "id": "g6aWb1e7tNRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_tokenizer = AutoTokenizer.from_pretrained(model_identifier, trust_remote_code = True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "xwOU4phefBFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Setting up the configuration for the LoRA fine-tuning method"
      ],
      "metadata": {
        "id": "yxOOwMvStaDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_setup = LoraConfig(lora_alpha = lora_hyper_alpha,\n",
        "                        lora_dropout = lora_hyper_dropout,\n",
        "                        r = lora_hyper_r,\n",
        "                        bias = \"none\",\n",
        "                        task_type = \"CAUSAL_LM\")"
      ],
      "metadata": {
        "id": "2hLqPh8zky3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Creating a training configuration by setting the training parameters"
      ],
      "metadata": {
        "id": "coUlIR-ytjiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = TrainingArguments(output_dir = results_dir,\n",
        "                               num_train_epochs = epochs_count,\n",
        "                               per_device_train_batch_size = train_batch_size,\n",
        "                               per_device_eval_batch_size = eval_batch_size,\n",
        "                               gradient_accumulation_steps = accumulation_steps,\n",
        "                               learning_rate = train_learning_rate,\n",
        "                               weight_decay = decay_rate,\n",
        "                               optim = optimizer_type,\n",
        "                               save_steps = checkpoint_interval,\n",
        "                               logging_steps = log_interval,\n",
        "                               fp16 = enable_fp16,\n",
        "                               bf16 = enable_bf16,\n",
        "                               max_grad_norm = grad_norm_limit,\n",
        "                               max_steps = steps_limit,\n",
        "                               warmup_ratio = warmup_percentage,\n",
        "                               group_by_length = length_grouping,\n",
        "                               lr_scheduler_type = scheduler_type,\n",
        "                               gradient_checkpointing = checkpointing_flag)"
      ],
      "metadata": {
        "id": "_XAv6G-bshio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Creating the Supervised Fine-Tuning Trainer"
      ],
      "metadata": {
        "id": "x-5-Thu0tpfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_sftt_trainer = SFTTrainer(model = llama_model,\n",
        "                                args = train_args,\n",
        "                                train_dataset = training_data,\n",
        "                                tokenizer = llama_tokenizer,\n",
        "                                peft_config = peft_setup,\n",
        "                                dataset_text_field = \"text\",\n",
        "                                max_seq_length = sequence_length_max,\n",
        "                                packing = enable_packing)"
      ],
      "metadata": {
        "id": "R-HP6oXEndmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Training the model"
      ],
      "metadata": {
        "id": "oSF8SHFKt1xL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_sftt_trainer.train()"
      ],
      "metadata": {
        "id": "qGM5nT5BuBmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 15: Chatting with the model"
      ],
      "metadata": {
        "id": "XMPw6WU6vbjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Please tell me about Bursitis\"\n",
        "text_generation_pipe = pipeline(task = \"text-generation\", model = llama_model, tokenizer = llama_tokenizer, max_length = 500)\n",
        "generation_result = text_generation_pipe(f\"<s>[INST] {user_prompt} [/INST]\")\n",
        "print(generation_result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "f3NQWGZP60Gt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}